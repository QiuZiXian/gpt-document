LMDeploy 量化部署进阶实践

## 1、环境准备

【30% A100的开发机】

```bash
conda create -n lmdeploy  python=3.10 -y
conda activate lmdeploy
conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia -y
pip install timm==1.0.8 openai==1.40.3 lmdeploy[all]==0.5.3
```

**环境安装卡半天？**

![image-20240830185123383](Lmdeploy%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E8%BF%9B%E9%98%B6.assets/image-20240830185123383.png)

安装比较慢，耐心等待

### 1.1 创建模型软链接

```bash
mkdir /root/models
ln -s /root/share/new_models/Shanghai_AI_Laboratory/internlm2_5-7b-chat /root/models
ln -s /root/share/new_models/Shanghai_AI_Laboratory/internlm2_5-1_8b-chat /root/models
ln -s /root/share/new_models/OpenGVLab/InternVL2-26B /root/models
```

![image-20240830183454558](Lmdeploy%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E8%BF%9B%E9%98%B6.assets/image-20240830183454558.png)

### 1.2 LMdeploy部署验证

```bash
lmdeploy chat /root/models/internlm2_5-7b-chat
```

![image-20240903101314701](Lmdeploy%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E8%BF%9B%E9%98%B6.assets/image-20240903101314701.png)

**GPU使用率**

![image-20240903101549940](Lmdeploy%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E8%BF%9B%E9%98%B6.assets/image-20240903101549940.png)

## 2、api接口服务

如果gpu内存不足，有可能是1.2的服务没有正常关闭，找到kill，释放gpu，如下图：

![image-20240903102601787](Lmdeploy%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E8%BF%9B%E9%98%B6.assets/image-20240903102601787.png)

### 2.1 启动API服务器

```bash
conda activate lmdeploy
lmdeploy serve api_server \
    /root/models/internlm2_5-7b-chat \
    --model-format hf \
    --quant-policy 0 \
    --server-name 0.0.0.0 \
    --server-port 23333 \
    --tp 1
```

命令解释：

1. `lmdeploy serve api_server`：这个命令用于启动API服务器。
2. `/root/models/internlm2_5-7b-chat`：这是模型的路径。
3. `--model-format hf`：这个参数指定了模型的格式。`hf`代表“Hugging Face”格式。
4. `--quant-policy 0`：这个参数指定了量化策略。
5. `--server-name 0.0.0.0`：这个参数指定了服务器的名称。在这里，`0.0.0.0`是一个特殊的IP地址，它表示所有网络接口。
6. `--server-port 23333`：这个参数指定了服务器的端口号。在这里，`23333`是服务器将监听的端口号。
7. `--tp 1`：这个参数表示并行数量（GPU数量）。



```bash
 # 本地cmd：
 ssh -CNg -L 23333:127.0.0.1:23333 root@ssh.intern-ai.org.cn -p 你的ssh端口号
```

然后打开浏览器，访问`http://127.0.0.1:23333`看到如下界面即代表部署成功。

![image-20240903103034783](Lmdeploy%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E8%BF%9B%E9%98%B6.assets/image-20240903103034783.png)

#### 2.1.1 以命令行形式连接API服务器

```bash
conda activate lmdeploy
lmdeploy serve api_client http://localhost:23333
```



![image-20240903103347539](Lmdeploy%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E8%BF%9B%E9%98%B6.assets/image-20240903103347539.png)

#### 2.1.2 以Gradio网页形式连接API服务器

```bash
# 开发机
lmdeploy serve gradio http://localhost:23333 \
    --server-name 0.0.0.0 \
    --server-port 6006
    
# 本地
ssh -CNg -L 6006:127.0.0.1:6006 root@ssh.intern-ai.org.cn -p <你的ssh端口号>
```

打开浏览器，访问地址`http://127.0.0.1:6006`，然后就可以与模型尽情对话了。

![image-20240903103654182](Lmdeploy%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E8%BF%9B%E9%98%B6.assets/image-20240903103654182.png)

### 2.2 LMDeploy Lite

随着模型变得越来越大，我们需要一些大模型压缩技术来降低模型部署的成本，并提升模型的推理性能。LMDeploy 提供了权重量化和 k/v cache两种策略。

kv cache是一种缓存技术，通过存储键值对的形式来复用计算结果，以达到提高性能和降低内存消耗的目的。在大规模训练和推理中，kv cache可以显著减少重复计算量，从而提升模型的推理速度。理想情况下，kv cache全部存储于显存，以加快访存速度。

模型在运行时，占用的显存可大致分为三部分：模型参数本身占用的显存、kv cache占用的显存，以及中间运算结果占用的显存。LMDeploy的kv cache管理器可以通过设置`--cache-max-entry-count`参数，控制kv缓存占用**剩余显存**的最大比例。默认的比例为0.8。

#### 2.2.1 不设置kv

参见1.2

![image-20240903104953243](Lmdeploy%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E8%BF%9B%E9%98%B6.assets/image-20240903104953243.png)

#### 2.2.2 设置kv

```bash
lmdeploy chat /root/models/internlm2_5-7b-chat --cache-max-entry-count 0.4
```

![image-20240903105156962](Lmdeploy%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E8%BF%9B%E9%98%B6.assets/image-20240903105156962.png)

#### 2.2.3 设置**在线** kv cache int4/int8 量化

```bash
lmdeploy serve api_server \
    /root/models/internlm2_5-7b-chat \
    --model-format hf \
    --quant-policy 4 \
    --cache-max-entry-count 0.4\
    --server-name 0.0.0.0 \
    --server-port 23333 \
    --tp 1
```

### 2.3 W4A16 模型量化和部署

准确说，模型量化是一种优化技术，旨在减少机器学习模型的大小并提高其推理速度。量化通过将模型的权重和激活从高精度（如16位浮点数）转换为低精度（如8位整数、4位整数、甚至二值网络）来实现。

那么标题中的W4A16又是什么意思呢？

- W4：这通常表示权重量化为4位整数（int4）。这意味着模型中的权重参数将从它们原始的浮点表示（例如FP32、BF16或FP16，**Internlm2.5精度为BF16**）转换为4位的整数表示。这样做可以显著减少模型的大小。
- A16：这表示激活（或输入/输出）仍然保持在16位浮点数（例如FP16或BF16）。激活是在神经网络中传播的数据，通常在每层运算之后产生。

因此，W4A16的量化配置意味着：

- 权重被量化为4位整数。
- 激活保持为16位浮点数。

```bash
lmdeploy lite auto_awq \
   /root/models/internlm2_5-1_8b-chat \
  --calib-dataset 'ptb' \
  --calib-samples 128 \
  --calib-seqlen 2048 \
  --w-bits 4 \
  --w-group-size 128 \
  --batch-size 1 \
  --search-scale False \
  --work-dir /root/models/internlm2_5-1_8b-chat-w4a16-4bit
```

命令解释：

1. `lmdeploy lite auto_awq`: `lite`这是LMDeploy的命令，用于启动量化过程，而`auto_awq`代表自动权重量化（auto-weight-quantization）。
2. `/root/models/internlm2_5-7b-chat`: 模型文件的路径。
3. `--calib-dataset 'ptb'`: 这个参数指定了一个校准数据集，这里使用的是’ptb’（Penn Treebank，一个常用的语言模型数据集）。
4. `--calib-samples 128`: 这指定了用于校准的样本数量—128个样本
5. `--calib-seqlen 2048`: 这指定了校准过程中使用的序列长度—2048
6. `--w-bits 4`: 这表示权重（weights）的位数将被量化为4位。
7. `--work-dir /root/models/internlm2_5-7b-chat-w4a16-4bit`: 这是工作目录的路径，用于存储量化后的模型和中间结果。

报错：

![image-20240903110016094](Lmdeploy%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E8%BF%9B%E9%98%B6.assets/image-20240903110016094.png)

临时解决：

```bash
vi /root/.conda/envs/lmdeploy/lib/python3.10/site-packages/datasets/load.py
```

![image-20240903110314954](Lmdeploy%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E8%BF%9B%E9%98%B6.assets/image-20240903110314954.png)

解决

![image-20240903110758804](Lmdeploy%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E8%BF%9B%E9%98%B6.assets/image-20240903110758804.png)

等待一段时间1-2个小时；

- 模型大小对比

![image-20240903123332270](Lmdeploy%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E8%BF%9B%E9%98%B6.assets/image-20240903123332270.png)

- 推理内存占用对比

  ```bash
  lmdeploy chat /root/models/internlm2_5-1_8b-chat-w4a16-4bit/ --model-format awq
  ```

  ![image-20240903123727480](Lmdeploy%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E8%BF%9B%E9%98%B6.assets/image-20240903123727480.png)

  量化前：

  ```bash
  conda activate lmdeploy
  lmdeploy chat /root/models/internlm2_5-1_8b-chat
  ```

  ![image-20240903124013996](Lmdeploy%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E8%BF%9B%E9%98%B6.assets/image-20240903124013996.png)

- 量化前：20616MiB
- 量化后：20196MiB

### 2.4 kv缓存+W4A16 量化同时设置

```bash
lmdeploy serve api_server \
    /root/models/internlm2_5-1_8b-chat-w4a16-4bit/ \
    --model-format awq \
    --quant-policy 4 \
    --cache-max-entry-count 0.4\
    --server-name 0.0.0.0 \
    --server-port 23333 \
    --tp 1
    
    
# 本地
 ssh -CNg -L 6006:127.0.0.1:6006 root@ssh.intern-ai.org.cn -p 你的ssh端口号
```

![image-20240903125315734](Lmdeploy%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E8%BF%9B%E9%98%B6.assets/image-20240903125315734.png)

## 3 LMDeploy与InternVL2

省略

## 4. LMDeploy之FastAPI与Function call

### 4.1 API开发

- 启动模型服务

```bash
# 2.4操作未关闭，可跳过
conda activate lmdeploy
lmdeploy serve api_server \
    /root/models/internlm2_5-1_8b-chat-w4a16-4bit \
    --model-format awq \
    --cache-max-entry-count 0.4 \
    --quant-policy 4 \
    --server-name 0.0.0.0 \
    --server-port 23333 \
    --tp 1
    
```

- api开发（代码调用）

  **可能出现的问题：**

  ![image-20240903130434650](Lmdeploy%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E8%BF%9B%E9%98%B6.assets/image-20240903130434650.png)

  **解决方案**：base_url="http://0.0.0.0:23333/v1" 改为base_url="http://127.0.0.1:23333/v1"

```bash
# 另启客户端
touch /root/internlm2_5.py
vi /root/internlm2_5.py

# internlm2_5.py
# 导入openai模块中的OpenAI类，这个类用于与OpenAI API进行交互
from openai import OpenAI


# 创建一个OpenAI的客户端实例，需要传入API密钥和API的基础URL
client = OpenAI(
    api_key='YOUR_API_KEY',  
    # 替换为你的OpenAI API密钥，由于我们使用的本地API，无需密钥，任意填写即可
    #base_url="http://0.0.0.0:23333/v1"  
    base_url="http://127.0.0.1:23333/v1"
    # 指定API的基础URL，这里使用了本地地址和端口
)

# 调用client.models.list()方法获取所有可用的模型，并选择第一个模型的ID
# models.list()返回一个模型列表，每个模型都有一个id属性
model_name = client.models.list().data[0].id

# 使用client.chat.completions.create()方法创建一个聊天补全请求
# 这个方法需要传入多个参数来指定请求的细节
response = client.chat.completions.create(
  model=model_name,  
  # 指定要使用的模型ID
  messages=[  
  # 定义消息列表，列表中的每个字典代表一个消息
    {"role": "system", "content": "你是一个友好的小助手，负责解决问题."},  
    # 系统消息，定义助手的行为
    {"role": "user", "content": "帮我讲述一个关于狐狸和西瓜的小故事"},  
    # 用户消息，询问时间管理的建议
  ],
    temperature=0.8,  
    # 控制生成文本的随机性，值越高生成的文本越随机
    top_p=0.8  
    # 控制生成文本的多样性，值越高生成的文本越多样
)

# 打印出API的响应结果
print(response.choices[0].message.content)
```

- 运行代码

```bash
conda activate lmdeploy
python /root/internlm2_5.py
```

- 输出结果

![image-20240903130615172](Lmdeploy%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E8%BF%9B%E9%98%B6.assets/image-20240903130615172.png)

### 4.2 Function call

- 启动7b, 1.8b的会报错(1.8b的可能不支持funtion)

![image-20240903131354198](Lmdeploy%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E8%BF%9B%E9%98%B6.assets/image-20240903131354198.png)

```bash
# 启动7b, 1.8b的会报错
conda activate lmdeploy
lmdeploy serve api_server \
    /root/models/internlm2_5-7b-chat \
    --model-format hf \
    --quant-policy 0 \
    --server-name 0.0.0.0 \
    --server-port 23333 \
    --tp 1
```

- 代码编制

```bash
touch /root/internlm2_5_func.py
vi /root/internlm2_5_func.py
#internlm2_5_func.py
from openai import OpenAI


def add(a: int, b: int):
    return a + b


def mul(a: int, b: int):
    return a * b


tools = [{
    'type': 'function',
    'function': {
        'name': 'add',
        'description': 'Compute the sum of two numbers',
        'parameters': {
            'type': 'object',
            'properties': {
                'a': {
                    'type': 'int',
                    'description': 'A number',
                },
                'b': {
                    'type': 'int',
                    'description': 'A number',
                },
            },
            'required': ['a', 'b'],
        },
    }
}, {
    'type': 'function',
    'function': {
        'name': 'mul',
        'description': 'Calculate the product of two numbers',
        'parameters': {
            'type': 'object',
            'properties': {
                'a': {
                    'type': 'int',
                    'description': 'A number',
                },
                'b': {
                    'type': 'int',
                    'description': 'A number',
                },
            },
            'required': ['a', 'b'],
        },
    }
}]
messages = [{'role': 'user', 'content': 'Compute (3+5)*2'}]

client = OpenAI(api_key='YOUR_API_KEY', base_url='http://0.0.0.0:23333/v1')
model_name = client.models.list().data[0].id
response = client.chat.completions.create(
    model=model_name,
    messages=messages,
    temperature=0.8,
    top_p=0.8,
    stream=False,
    tools=tools)
print(response)
func1_name = response.choices[0].message.tool_calls[0].function.name
func1_args = response.choices[0].message.tool_calls[0].function.arguments
func1_out = eval(f'{func1_name}(**{func1_args})')
print(func1_out)

messages.append({
    'role': 'assistant',
    'content': response.choices[0].message.content
})
messages.append({
    'role': 'environment',
    'content': f'3+5={func1_out}',
    'name': 'plugin'
})
response = client.chat.completions.create(
    model=model_name,
    messages=messages,
    temperature=0.8,
    top_p=0.8,
    stream=False,
    tools=tools)
print(response)
func2_name = response.choices[0].message.tool_calls[0].function.name
func2_args = response.choices[0].message.tool_calls[0].function.arguments
func2_out = eval(f'{func2_name}(**{func2_args})')
print(func2_out)
```

- 运行代码

```bash
python /root/internlm2_5_func.py
```

- 结果展示

![image-20240903131457441](Lmdeploy%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E8%BF%9B%E9%98%B6.assets/image-20240903131457441.png)